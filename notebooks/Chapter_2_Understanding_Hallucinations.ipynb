{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Chapter 2: Understanding Hallucinations in Large Language Models (LLMs)\n",
    "\n",
    "This chapter explores hallucinations in Large Language Models (LLMs), focusing on how and why they occur, and providing practical examples to help you understand this phenomenon. Hallucination refers to instances where the model generates plausible-sounding but factually incorrect or fabricated information.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Define and understand the concept of hallucinations in LLMs.\n",
    "- Identify common causes of hallucinations.\n",
    "- Observe hallucination behaviours through hands-on examples.\n",
    "- Experiment with model prompts to observe how certain inputs can induce hallucinations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is Hallucination?\n",
    "\n",
    "In the context of LLMs, **hallucination** occurs when a model produces output that is plausible but factually incorrect or fabricated. This behaviour can be particularly problematic in applications where accuracy is crucial, such as healthcare or finance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Common Causes of Hallucinations\n",
    "\n",
    "Some of the primary reasons hallucinations occur in LLMs include:\n",
    "\n",
    "- **Incomplete or Biased Training Data**: If a model is trained on incomplete or biased data, it may generate responses that reflect gaps or biases in that data.\n",
    "- **Overfitting**: When a model becomes overly familiar with specific data patterns, it may produce answers based on those patterns, even if they don’t fully apply.\n",
    "- **Prompt Ambiguity**: Vague or unclear prompts can lead to misinterpretation, causing the model to make assumptions and \"fill in the gaps\" with fabricated content.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Code Example: Observing Hallucination Behaviour\n",
    "\n",
    "In this example, we’ll use OpenAI’s **GPT-4** (via the OpenAI API) and Meta’s **LLaMA** model (via the `transformers` library) to observe how hallucination might occur. We'll test with ambiguous prompts to see if the models generate information that isn’t factually accurate.\n",
    "\n",
    "---\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "Install the necessary libraries before running the code.\n",
    "\n",
    "```python\n",
    "!pip install openai transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt likely to induce hallucination\n",
    "ambiguous_prompt = \"The first person to walk on Mars was\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Observing Hallucinations in GPT-4 via OpenAI API\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"  # Replace with your OpenAI API key\n",
    "\n",
    "def gpt4_hallucinate(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hallucination with GPT-4\n",
    "print(\"GPT-4 Response to Ambiguous Prompt:\")\n",
    "print(gpt4_hallucinate(ambiguous_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Observing Hallucinations in Meta’s LLaMA via Hugging Face\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
    "\n",
    "def llama_hallucinate(prompt):\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = llama_model.generate(inputs[\"input_ids\"], max_length=30, num_return_sequences=1)\n",
    "    return llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hallucination with LLaMA\n",
    "print(\"\\nLLaMA Response to Ambiguous Prompt:\")\n",
    "print(llama_hallucinate(ambiguous_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "After running the code above, observe the following:\n",
    "\n",
    "1. **Response Content**: Did the model produce a factual answer, or did it fabricate information about a Mars landing that hasn't happened yet?\n",
    "\n",
    "2. **Pattern Recognition**: Notice if the model tries to \"fill in\" details based on patterns it learned, such as names or dates, that aren’t grounded in reality.\n",
    "\n",
    "3. **Difference Between Models**: Compare how GPT-4 and LLaMA handle the ambiguous prompt. Are there differences in the tendency of each model to hallucinate?\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Exercise: Experimenting with Hallucinations Using Different Prompts\n",
    "\n",
    "This exercise will help you understand how prompt phrasing can influence hallucination. You’ll try different types of prompts and observe how the models respond.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Choose a Set of Ambiguous Prompts**: Experiment with prompts that are likely to cause hallucinations. Examples:\n",
    "   - \"The largest animal discovered in the Amazon rainforest is\"\n",
    "   - \"The cure for all cancers was discovered by\"\n",
    "   - \"The first spacecraft to reach another galaxy was called\"\n",
    "\n",
    "2. **Run Each Prompt with Both Models**: Test each prompt with GPT-4 and LLaMA, as shown in the code example above.\n",
    "\n",
    "3. **Record Your Observations**: For each prompt, note whether the model produced accurate information or hallucinated, and if so, how convincingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts for experimentation\n",
    "prompts = [\n",
    "    \"The largest animal discovered in the Amazon rainforest is\",\n",
    "    \"The cure for all cancers was discovered by\",\n",
    "    \"The first spacecraft to reach another galaxy was called\"\n",
    "]\n",
    "\n",
    "print(\"Observations with GPT-4o:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"GPT-4o Response:\", gpt4o_hallucinate(prompt))\n",
    "\n",
    "print(\"\\nObservations with LLaMA:\")\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"LLaMA Response:\", llama_hallucinate(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Observations\n",
    "\n",
    "For each prompt, observe:\n",
    "\n",
    "1. **Accuracy**: Did the model respond with known facts, or did it create fictional information?\n",
    "2. **Consistency**: Are the hallucinated details consistent across different prompts or randomised?\n",
    "3. **Model Comparison**: Note any differences between GPT-4 and LLaMA in terms of hallucination tendencies and response quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "- Hallucinations in LLMs occur when the model generates plausible-sounding but incorrect information.\n",
    "- Common causes include incomplete training data, model overfitting, and prompt ambiguity.\n",
    "- Observing hallucinations helps us understand where and why they might arise, leading to insights into ways to mitigate them.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "In this chapter, we defined hallucinations in LLMs and explored their causes. We observed hallucinations in real-time with ambiguous prompts, using the latest models—GPT-4 and LLaMA—to better understand how they respond to unclear inputs. Through exercises, we explored different prompts to see how easily these models might \"hallucinate\" or fabricate information when faced with limited or ambiguous context.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
