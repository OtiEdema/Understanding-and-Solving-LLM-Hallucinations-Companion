{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Case Studies on Hallucinations in LLM Applications\n",
    "\n",
    "In this chapter, we will explore real-world applications of Large Language Models (LLMs) where hallucinations could present significant challenges. Case studies in fields such as healthcare, finance, and legal applications illustrate why hallucination mitigation is essential in high-stakes environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Understand the potential impacts of hallucinations in specific fields.\n",
    "- Identify why certain applications are more susceptible to hallucination risks.\n",
    "- Implement techniques that reduce hallucinations in practical use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Healthcare Case Study: Medical Advice Assistant\n",
    "\n",
    "LLMs can assist in healthcare by answering medical questions, suggesting treatments, or summarising patient notes. However, hallucinations can lead to incorrect medical advice, which could be dangerous for patients.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Observing Hallucinations in a Healthcare Scenario\n",
    "\n",
    "Let's simulate a medical advice assistant using GPT-4 and LLaMA. We’ll use ambiguous prompts to see if hallucinations occur in medical contexts.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "Ensure the necessary libraries are installed before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Code Implementation: Simulating a Medical Advice Assistant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API for GPT-4\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"  # Replace with your OpenAI API key\n",
    "\n",
    "def gpt4_medical_response(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLaMA model for medical prompts\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
    "\n",
    "def llama_medical_response(prompt):\n",
    "    inputs = llama_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = llama_model.generate(inputs[\"input_ids\"], max_length=50, num_return_sequences=1)\n",
    "    return llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ambiguous medical prompts\n",
    "prompt = \"What are the best treatments for chronic pain?\"\n",
    "followup_prompt = \"Is it safe to take aspirin if you have diabetes?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test responses with GPT-4\n",
    "print(\"GPT-4 Response to Medical Prompt:\")\n",
    "print(gpt4_medical_response(prompt))\n",
    "print(\"\\nGPT-4 Response to Follow-Up Medical Prompt:\")\n",
    "print(gpt4_medical_response(followup_prompt))\n",
    "\n",
    "# Test responses with LLaMA\n",
    "print(\"\\nLLaMA Response to Medical Prompt:\")\n",
    "print(llama_medical_response(prompt))\n",
    "print(\"\\nLLaMA Response to Follow-Up Medical Prompt:\")\n",
    "print(llama_medical_response(followup_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "After running the code above, observe the following:\n",
    "\n",
    "1. **Medical Accuracy**: Did the model provide safe, accurate medical information, or did it produce any potentially unsafe advice?\n",
    "2. **Pattern Recognition**: Notice if the model fills in information that sounds plausible but may not be factually correct.\n",
    "3. **Model Comparison**: Compare how GPT-4 and LLaMA handle ambiguous medical prompts. Does one model show a higher tendency to hallucinate?\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Financial Case Study: Investment Advice Assistant\n",
    "\n",
    "In finance, LLMs can be used to provide investment insights or summarise market trends. Hallucinations in this field could lead to inaccurate advice, potentially resulting in financial losses.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Simulating Financial Advice\n",
    "\n",
    "In this example, we’ll test how models respond to prompts about investment advice. Financial advice requires factual and reliable information, as incorrect advice could be costly.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation: Simulating Financial Advice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define financial prompts\n",
    "financial_prompt = \"What are the best stocks to invest in right now?\"\n",
    "risk_assessment_prompt = \"Is investing in tech stocks safe during a recession?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test responses with GPT-4\n",
    "print(\"GPT-4 Response to Financial Prompt:\")\n",
    "print(gpt4_medical_response(financial_prompt))\n",
    "print(\"\\nGPT-4 Response to Risk Assessment Prompt:\")\n",
    "print(gpt4_medical_response(risk_assessment_prompt))\n",
    "\n",
    "# Test responses with LLaMA\n",
    "print(\"\\nLLaMA Response to Financial Prompt:\")\n",
    "print(llama_medical_response(financial_prompt))\n",
    "print(\"\\nLLaMA Response to Risk Assessment Prompt:\")\n",
    "print(llama_medical_response(risk_assessment_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "For each response, observe:\n",
    "\n",
    "1. **Accuracy in Financial Advice**: Does the model provide realistic, well-researched investment advice, or does it fabricate information?\n",
    "2. **Safety of Advice**: Note if the model’s advice would lead to risky financial decisions without proper disclaimers.\n",
    "3. **Comparative Analysis**: Assess which model is more prone to generating unsubstantiated or risky financial advice.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Legal Case Study: Legal Assistance Chatbot\n",
    "\n",
    "In legal applications, LLMs might help users understand legal terms or offer guidance on common questions. However, hallucinations can lead to inaccurate legal interpretations, which could mislead users and cause legal complications.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Simulating a Legal Assistance Chatbot\n",
    "\n",
    "In this case study, we’ll test how models respond to legal questions and if they provide accurate or speculative answers.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Implementation: Simulating Legal Assistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define legal prompts\n",
    "legal_prompt = \"Can I get a divorce without a lawyer?\"\n",
    "followup_legal_prompt = \"What are the tax implications of divorce settlements?\"\n",
    "\n",
    "# Test responses with GPT-4\n",
    "print(\"GPT-4 Response to Legal Prompt:\")\n",
    "print(gpt4_medical_response(legal_prompt))\n",
    "print(\"\\nGPT-4 Response to Follow-Up Legal Prompt:\")\n",
    "print(gpt4_medical_response(followup_legal_prompt))\n",
    "\n",
    "# Test responses with LLaMA\n",
    "print(\"\\nLLaMA Response to Legal Prompt:\")\n",
    "print(llama_medical_response(legal_prompt))\n",
    "print(\"\\nLLaMA Response to Follow-Up Legal Prompt:\")\n",
    "print(llama_medical_response(followup_legal_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "For each legal prompt, observe:\n",
    "\n",
    "1. **Legal Accuracy**: Did the model provide accurate information, or did it produce any speculative or misleading responses?\n",
    "2. **Adherence to Jurisdiction**: Check if the model provides advice that could vary by jurisdiction, making it potentially inaccurate for certain users.\n",
    "3. **Comparison Across Models**: Analyse if one model is more prone to hallucinating legal information or if both exhibit similar tendencies.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise: Testing Model Hallucinations in Real-World Scenarios\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Create New Prompts for Each Case Study**: Write your own prompts for healthcare, finance, and legal fields. For example:\n",
    "   - Healthcare: \"What is the best diet for managing hypertension?\"\n",
    "   - Finance: \"Should I invest in real estate or cryptocurrency?\"\n",
    "   - Legal: \"Do I need to report foreign income on my taxes?\"\n",
    "\n",
    "2. **Test with Both Models**: Use GPT-4 and LLaMA to respond to these prompts.\n",
    "\n",
    "3. **Record Your Observations**: For each response, note if the model hallucinated information, provided accurate advice, or produced vague answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example custom prompts\n",
    "custom_prompts = [\n",
    "    \"What is the best diet for managing hypertension?\",\n",
    "    \"Should I invest in real estate or cryptocurrency?\",\n",
    "    \"Do I need to report foreign income on my taxes?\"\n",
    "]\n",
    "\n",
    "print(\"Observations with GPT-4:\")\n",
    "for prompt in custom_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"GPT-4 Response:\", gpt4_medical_response(prompt))\n",
    "\n",
    "print(\"\\nObservations with LLaMA:\")\n",
    "for prompt in custom_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"LLaMA Response:\", llama_medical_response(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this chapter, we examined real-world case studies where hallucinations in LLMs can have significant consequences:\n",
    "\n",
    "1. **Healthcare**: Hallucinations can lead to unsafe medical advice.\n",
    "2. **Finance**: Inaccurate investment advice could lead to financial losses.\n",
    "3. **Legal**: Incorrect legal guidance can mislead users and cause legal issues.\n",
    "\n",
    "These case studies highlight the importance of mitigating hallucinations in high-stakes applications, where reliable and accurate information is critical.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
