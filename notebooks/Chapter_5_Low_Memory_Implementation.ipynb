{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Low Memory Implementation Techniques\n",
    "\n",
    "This chapter focuses on strategies for reducing the memory footprint of Large Language Models (LLMs), such as quantisation and pruning. These techniques enable the deployment of LLMs in resource-constrained environments without significantly sacrificing performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Understand the importance of low memory techniques for deploying LLMs.\n",
    "- Implement dynamic quantisation to reduce model size.\n",
    "- Explore pruning techniques to optimise model parameters.\n",
    "- Apply these techniques in practical scenarios for efficient LLM deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Low Memory Techniques\n",
    "\n",
    "Large Language Models often require significant memory and computational resources, which can make them impractical for certain applications. Low memory techniques, such as quantisation and pruning, address this challenge by optimising the model’s size and performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Quantisation: Reducing Model Precision\n",
    "\n",
    "### What is Quantisation?\n",
    "\n",
    "Quantisation reduces the precision of a model’s weights (e.g., from 32-bit floating point to 8-bit integers), resulting in smaller model sizes and faster computations.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example: Applying Dynamic Quantisation\n",
    "\n",
    "Dynamic quantisation is one of the simplest methods to reduce model size without retraining. Let's apply it to a pre-trained model like LLaMA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLaMA model\n",
    "model_name = \"meta-llama/Llama-2-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the original model size\n",
    "original_size = sum(param.numel() for param in model.parameters()) * 4 / 1e6  # Float32 = 4 bytes\n",
    "print(f\"Original model size: {original_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic quantisation\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the quantised model size\n",
    "quantized_size = sum(param.numel() for param in quantized_model.parameters()) * 1 / 1e6  # Int8 = 1 byte\n",
    "print(f\"Quantised model size: {quantized_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantised model\n",
    "quantized_model.save_pretrained(\"./quantized_llama_model\")\n",
    "print(\"\\nQuantised model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "After running the code above:\n",
    "1. **Size Reduction**: Note the difference in model size before and after quantisation.\n",
    "2. **Performance Trade-Offs**: Dynamic quantisation typically results in negligible accuracy loss for most text generation tasks. Observe if there are any noticeable trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Pruning: Removing Redundant Parameters\n",
    "\n",
    "### What is Pruning?\n",
    "\n",
    "Pruning involves removing less significant weights or neurons from the model to reduce its complexity and size. This technique is useful for fine-tuning a model for specific tasks while maintaining efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example: Basic Weight Pruning\n",
    "\n",
    "Let's apply weight pruning to the LLaMA model to reduce its memory requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the LLaMA model\n",
    "model_name = \"meta-llama/Llama-2-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Prune 30% of the weights in all linear layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.3)\n",
    "\n",
    "# Check the model after pruning\n",
    "print(\"Model after pruning:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        print(f\"{name}: {module.weight}\")\n",
    "\n",
    "# Save the pruned model\n",
    "model.save_pretrained(\"./pruned_llama_model\")\n",
    "print(\"\\nPruned model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Observations\n",
    "\n",
    "1. **Weight Sparsity**: Observe the sparsity of weights after pruning. Many weights will be set to zero.\n",
    "2. **Impact on Model Performance**: While pruning reduces model size, excessive pruning can degrade performance. Experiment with different pruning percentages (e.g., 10%, 30%, 50%).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Exercise: Combining Quantisation and Pruning\n",
    "\n",
    "In this exercise, you will combine quantisation and pruning to further optimise the LLaMA model.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Apply Pruning**: Start by pruning 20% of the weights in the linear layers.\n",
    "2. **Quantise the Pruned Model**: Apply dynamic quantisation to the pruned model.\n",
    "3. **Compare Model Sizes**: Measure the size of the model before and after applying both techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pruning\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name=\"weight\", amount=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantisation to the pruned model\n",
    "pruned_quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final model size\n",
    "final_size = sum(param.numel() for param in pruned_quantized_model.parameters()) * 1 / 1e6\n",
    "print(f\"Final model size after pruning and quantisation: {final_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "pruned_quantized_model.save_pretrained(\"./pruned_quantized_llama_model\")\n",
    "print(\"\\nPruned and quantised model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "- **Quantisation** reduces model size and computation costs by lowering weight precision.\n",
    "- **Pruning** eliminates redundant parameters, making the model more efficient.\n",
    "- **Combining Techniques**: Combining quantisation and pruning maximises memory savings while maintaining acceptable performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "In this chapter, we explored techniques for reducing the memory footprint of LLMs:\n",
    "\n",
    "1. **Quantisation**: Reduced model precision to save memory.\n",
    "2. **Pruning**: Removed redundant parameters for efficiency.\n",
    "3. **Combined Optimisation**: Combined quantisation and pruning to achieve significant memory savings.\n",
    "\n",
    "These techniques enable the deployment of LLMs in resource-constrained environments, making them accessible to a wider range of applications.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
