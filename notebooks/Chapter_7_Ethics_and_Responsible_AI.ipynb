{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Ethics and Responsible AI in LLM Deployment\n",
    "\n",
    "The use of Large Language Models (LLMs) comes with significant ethical responsibilities. This chapter explores key ethical principles and practices for deploying LLMs responsibly, ensuring fairness, transparency, accountability, and privacy.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Understand the ethical challenges of deploying LLMs in real-world applications.\n",
    "- Implement guidelines to promote fairness, transparency, and accountability.\n",
    "- Apply privacy-preserving techniques to protect user data.\n",
    "- Explore tools and libraries that support ethical AI development.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Key Ethical Challenges in LLM Deployment\n",
    "\n",
    "LLMs are powerful tools but can pose risks if not handled responsibly. Key ethical challenges include:\n",
    "\n",
    "1. **Bias**: Models trained on biased datasets may perpetuate or amplify stereotypes.\n",
    "2. **Hallucination**: Fabricated information can mislead users, especially in high-stakes domains like healthcare or law.\n",
    "3. **Privacy**: User data processed by LLMs must be handled securely to prevent breaches or misuse.\n",
    "4. **Accountability**: Determining who is responsible for model outputs can be challenging, particularly for automated decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Observing Bias in LLM Outputs\n",
    "\n",
    "We’ll explore how biased training data can influence the outputs of an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define prompts with potential for bias\n",
    "prompts = [\n",
    "    \"Men are better at\",\n",
    "    \"Women are better at\",\n",
    "    \"The ideal leader is\"\n",
    "]\n",
    "\n",
    "# Generate responses for each prompt\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=20, num_return_sequences=1)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Observations\n",
    "\n",
    "1. **Bias in Outputs**: Note if the model generates stereotypical or biased responses.\n",
    "2. **Prompt Sensitivity**: Observe how slight changes in the prompt affect the bias in the responses.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Promoting Fairness in LLMs\n",
    "\n",
    "**Fairness** in AI means ensuring that models do not discriminate against any individual or group. This can be achieved through:\n",
    "\n",
    "1. **Diverse Training Data**: Use balanced datasets that represent different demographics and perspectives.\n",
    "2. **Debiasing Techniques**: Apply algorithms to identify and mitigate bias in the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Using Fairness Libraries\n",
    "\n",
    "Libraries like `AIF360` can help evaluate and mitigate bias in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AIF360 (if not already installed)\n",
    "# !pip install aif360\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Create a synthetic dataset for demonstration\n",
    "data = {\n",
    "    'feature': [0, 1, 0, 1, 0, 1],\n",
    "    'label': [1, 0, 1, 0, 1, 0],\n",
    "    'protected_attribute': [1, 1, 0, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Load data into BinaryLabelDataset\n",
    "dataset = BinaryLabelDataset(df=pd.DataFrame(data), label_names=['label'], protected_attribute_names=['protected_attribute'])\n",
    "\n",
    "# Evaluate fairness metrics\n",
    "metric = BinaryLabelDatasetMetric(dataset, privileged_groups=[{'protected_attribute': 1}], unprivileged_groups=[{'protected_attribute': 0}])\n",
    "print(\"Disparate impact ratio:\", metric.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Ensuring Transparency and Accountability\n",
    "\n",
    "Transparency ensures users understand how the model works and its limitations. Accountability assigns responsibility for the model’s behaviour and decisions.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Model Documentation**: Provide detailed documentation, including:\n",
    "   - Training data sources\n",
    "   - Known limitations\n",
    "   - Use cases and restrictions\n",
    "2. **Explainability**: Use tools to make model predictions interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Using Explainability Tools\n",
    "\n",
    "Libraries like `SHAP` and `LIME` can help explain model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP (if not already installed)\n",
    "# !pip install shap\n",
    "\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "# Tokenise input for SHAP explanation\n",
    "input_text = \"Explainable AI is important.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(model, tokenizer)\n",
    "shap_values = explainer(inputs)\n",
    "\n",
    "# Visualise SHAP values\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install SHAP (if not already installed)\n",
    "# !pip install shap\n",
    "\n",
    "import shap\n",
    "import torch\n",
    "\n",
    "# Tokenise input for SHAP explanation\n",
    "input_text = \"Explainable AI is important.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(model, tokenizer)\n",
    "shap_values = explainer(inputs)\n",
    "\n",
    "# Visualise SHAP values\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Privacy-Preserving Techniques\n",
    "\n",
    "Privacy is critical when handling sensitive user data. Techniques for preserving privacy include:\n",
    "\n",
    "1. **Data Anonymisation**: Remove personally identifiable information (PII) from datasets.\n",
    "2. **Differential Privacy**: Add noise to data or model outputs to prevent individual identification.\n",
    "3. **Federated Learning**: Train models on local devices without sharing raw data.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Implementing Differential Privacy\n",
    "\n",
    "We'll use `Opacus`, a library for differential privacy in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Opacus (if not already installed)\n",
    "# !pip install opacus\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from transformers import AdamW\n",
    "\n",
    "# Load a simple model and data\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Attach PrivacyEngine for differential privacy\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=None,  # Replace with your DataLoader\n",
    "    target_epsilon=1.0,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "print(\"Model training with differential privacy enabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "- Ethical AI deployment requires addressing bias, ensuring transparency, and preserving privacy.\n",
    "- Tools like `AIF360`, `SHAP`, and `Opacus` support responsible AI development.\n",
    "- Documentation and explainability are critical for building trust with users.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "In this chapter, we explored ethical considerations for deploying LLMs responsibly:\n",
    "\n",
    "1. **Bias Mitigation**: Ensuring fairness through diverse data and debiasing techniques.\n",
    "2. **Transparency**: Providing clear documentation and explainability.\n",
    "3. **Privacy Preservation**: Using anonymisation, differential privacy, and federated learning.\n",
    "\n",
    "These practices ensure LLMs are deployed in a manner that respects users' rights and builds trust in AI systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
